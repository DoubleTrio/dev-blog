[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GSOC 2025 - Pitch Correction for Sound Playback in Sequencer Blog",
    "section": "",
    "text": "Understanding VSE Audio in the Sequencer\n\n\n\nblender\n\nc++\n\n\n\nGeneral notes on how audio and pitch works in Blender’s VSE\n\n\n\n\n\nJun 7, 2025\n\n\nTheKaceFiles\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Signal Notes\n\n\n\ndsp\n\npython\n\n\n\nA collection of Digital Signal Processing notes\n\n\n\n\n\nMay 24, 2025\n\n\nTheKaceFiles\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMay 23, 2025\n\n\nTheKaceFiles\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/signal_review/index.html",
    "href": "posts/signal_review/index.html",
    "title": "Random Signal Notes",
    "section": "",
    "text": "We’ll first import the following libraries:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio\n\n\nSuppose we have the following signal which is sampled at 50 Hz and has a duration of 2 seconds.\n\n\nCode\nfs = 50\nduration = 2\nfreq = 2\nN = np.arange(int(fs * duration))\nt = N / fs       \ny = np.cos(2 * np.pi * freq * t)\n\n\nplt.plot(t, y)\nplt.xlabel(\"Seconds\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"Wave Example\")\n\nplt.grid(True)\n\n\n\n\n\n\n\n\nFigure 1: Signal example\n\n\n\n\n\nThe signal above has a frequency of 2 Hz (fs) which means that it completes 2 cycles in 1 second.\n\n\n\n\n\n\nDefinition: Period\n\n\n\nThe period of the signal is how long it takes for a signal to complete 1 cycle by taking\n\\[\nt_0 = \\frac{1}{f_s}\n\\]\nwhere \\(f_s\\) is the frequency of the signal.\n\n\nIn Figure 1, the signal has a period of \\(t_0 = \\frac{1}{f_s} = \\frac{1}{2} = 0.5\\) seconds."
  },
  {
    "objectID": "posts/signal_review/index.html#signals",
    "href": "posts/signal_review/index.html#signals",
    "title": "Random Signal Notes",
    "section": "",
    "text": "We’ll first import the following libraries:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio\n\n\nSuppose we have the following signal which is sampled at 50 Hz and has a duration of 2 seconds.\n\n\nCode\nfs = 50\nduration = 2\nfreq = 2\nN = np.arange(int(fs * duration))\nt = N / fs       \ny = np.cos(2 * np.pi * freq * t)\n\n\nplt.plot(t, y)\nplt.xlabel(\"Seconds\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"Wave Example\")\n\nplt.grid(True)\n\n\n\n\n\n\n\n\nFigure 1: Signal example\n\n\n\n\n\nThe signal above has a frequency of 2 Hz (fs) which means that it completes 2 cycles in 1 second.\n\n\n\n\n\n\nDefinition: Period\n\n\n\nThe period of the signal is how long it takes for a signal to complete 1 cycle by taking\n\\[\nt_0 = \\frac{1}{f_s}\n\\]\nwhere \\(f_s\\) is the frequency of the signal.\n\n\nIn Figure 1, the signal has a period of \\(t_0 = \\frac{1}{f_s} = \\frac{1}{2} = 0.5\\) seconds."
  },
  {
    "objectID": "posts/signal_review/index.html#human-hearing",
    "href": "posts/signal_review/index.html#human-hearing",
    "title": "Collection of Random Signal Notes",
    "section": "Human Hearing",
    "text": "Human Hearing\nThe human ear can hear frequencies between 20 Hz and 20 kHz. The sampling rate is 44.1 kHZ because of something called the Nyquist–Shannon sampling theorem which states that the sampling rate has to be at least twice of the maximum frequency of the signal.\n\n\nCode\nfs = 44100\n\nduration = 3\nN = int(duration * fs)\nfreq = 440\n\nn = np.arange(N)\nt = n / fs       \nx = np.cos(2 * np.pi * freq * n / fs)\n\nAudio(data=x, rate=fs)\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/prelude/index.html",
    "href": "posts/prelude/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Hi! I’m the TheKaceFiles and I’m currently one of the GSOC contributors for Blender! This summer, I’ll be implementing a pitch correction toggle for Blender’s Video Sequence Editor (VSE) under Aras Pranckevičius. This blog will be primarily used for notes covering Blender’s codebase, the pitch correction research papers, review concepts from digital signal processing, whatever topics I find interesting, or just my thinking process. Do note that my thoughts/notes will not be always be organized or precise, but they will be reorganized once I have a better grasp.\nI will admit, I do feel both nervous and excited for this project. Nervous, as in, I’m worried that I will be unable to complete the project within the upcoming months and that this is one of the biggest project/feature I will be undertaking. Excited, as in, I will be learning (and relearning) many new things like concepts for pitch correction and navigating Blender’s codebase. Over the next few days/weeks, I will reviewing and Digital Signals Theory by Brian McFee and A Digital Signal Processing Primer by Kenneth Steiglitz and putting up my own notes up. One of the biggest gripes I have about these books is that they never have enough examples or solutions (which is understandable as they’re trying to minimize the page count for printing!) My notes are meant to bridge this gap and meant to help me (and hopefully others) better understand the material!\nWe’ll see how that goes until then!"
  },
  {
    "objectID": "posts/signal_review/index.html#notes-on-hearing",
    "href": "posts/signal_review/index.html#notes-on-hearing",
    "title": "Collection of Random Signal Notes",
    "section": "Notes on Hearing",
    "text": "Notes on Hearing\nThe human ear can hear frequencies between 20 Hz and 20 kHz. The sampling rate is 44.1 kHZ because of something called the Nyquist–Shannon sampling theorem which states that the sampling rate has to be at least twice of the maximum frequency of the signal which indeed \\(2 \\cdot 20 \\text{ kHz} &lt; 44.1 \\text{ kHZ}\\). There’s also more history for why the samplign rate is 44.1 kHZ here.\nNow,\nA3 = 220 Hz\nA4 = 440 Hz\nA5 = 880 Hz\n\\[\nf = 2^{(n/12)} * 440\n\\]\n\n\nCode\nfs = 44100\n\nduration = 3\nN = int(duration * fs)\nfreq = 440\n\nn = np.arange(N)\nt = n / fs       \nx = np.cos(2 * np.pi * freq * n / fs)\n\nAudio(data=x, rate=fs)\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/signal_review/index.html#notes-on-human-hearing",
    "href": "posts/signal_review/index.html#notes-on-human-hearing",
    "title": "Collection of Random Signal Notes",
    "section": "Notes on Human Hearing",
    "text": "Notes on Human Hearing\nThe human ear can hear frequencies between 20 Hz and 20 kHz. The sampling rate is 44.1 kHZ because of something called the Nyquist–Shannon sampling theorem which states that the sampling rate has to be at least twice of the maximum frequency of the signal which indeed \\(2 \\cdot 20 \\text{ kHz} &lt; 44.1 \\text{ kHZ}\\). There’s also more history for why the sampling rate is 44.1 kHZ here.\nNow, as for the frequencies of musical notes in Western music, we can use the formula below:\n\\[\nf = 2^{(n/12)} * 440\n\\]\nThe formula represents the frequency of the music note that is \\(n\\) semitones away from A4 (440 Hz). Each increasing semitone step increases the frequency by the ratio of \\(2^{(n/12)}\\).\n\n\n\n\n\n\nExamples\n\n\n\n\\(n = 0 \\rightarrow 2^{(0/12)} * 440 = 440 \\text{ Hz} = A4\\)\n\\(n = 1 \\rightarrow 2^{(1/12)} * 440 \\approx 466.163 \\text{ Hz} = A \\sharp 4 / B\\flat 4\\)\n\\(n = 12 \\rightarrow 2^{(12/12)} * 440 = 880 \\text{ Hz} = A5\\)\n\\(n = -12 \\rightarrow 2^{(-12/12)} * 440 = 220 \\text{ Hz} = A3\\)\n\n\nNotice that doubling the frequency represents an octave increase of a music note. Now, if we assume that 20 kHz is the maximum frequency that the human ear can hear, then the highest named musical note can be found by solving for \\(n\\) when \\(f =  20 \\text{ kHz}\\).\n\\[\n\\begin{align*}\n20000 &= 2^{\\frac{n}{12}} \\cdot 440 \\\\\n\\frac{20000}{440} &= 2^{\\frac{n}{12}} \\\\\n45.45 &= 2^{\\frac{n}{12}} \\\\\n\\log_2(45.45) &= \\frac{n}{12} \\\\\nn &= 12 \\cdot \\log_2(45.45) \\\\\nn &\\approx 66.0745\n\\end{align*}\n\\]\nWhen \\(n \\approx 66\\), this is 66 semitones aboves A4 and corresponds to the note \\(D \\sharp 10/ E\\flat 10\\).\nLet’s take a listen of the music notes, starting from A3 to D#10!\n\n\nCode\nfs = 44100\nnote_duration = 0.5\npause_duration = 0.10\n\nN = np.arange(int(fs * note_duration))\nt = N / fs\n\npause_arr = np.zeros(int(fs * pause_duration))\n\ndef get_frequency(n):\n    \"\"\"\n    Return the frequency of the note n semitones starting from A4 (440 Hz).\n    \"\"\"\n    return 2**(n / 12) * 440\n\n\n\naudio_sequence = np.array([])\nfor n in range(-12, 67):\n  freq = get_frequency(n)\n  signal_arr = np.cos(2 * np.pi * freq * t)\n  audio_sequence = np.concatenate((audio_sequence, signal_arr, pause_arr))\n\nAudio(data=audio_sequence, rate=fs)\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/signal_review/index.html#notes-on-music-notes",
    "href": "posts/signal_review/index.html#notes-on-music-notes",
    "title": "Random Signal Notes",
    "section": "Notes on Music Notes",
    "text": "Notes on Music Notes\nThe human ear can hear frequencies between 20 Hz and 20 kHz. The sampling rate is 44.1 kHZ because of something called the Nyquist–Shannon sampling theorem which states that the sampling rate has to be at least twice of the maximum frequency of the signal which indeed \\(2 \\cdot 20 \\text{ kHz} &lt; 44.1 \\text{ kHZ}\\). There’s also more history for why the sampling rate is 44.1 kHZ here.\nNow, as for the frequencies of musical notes in Western music, we can use the formula below:\n\\[\nf = 2^{(n/12)} * 440\n\\]\nThe formula represents the frequency of the music note that is \\(n\\) semitones away from A4 (440 Hz). Each increasing semitone step increases the frequency by the ratio of \\(2^{(n/12)}\\).\n\n\n\n\n\n\nExamples\n\n\n\n\\(n = 0 \\rightarrow 2^{(0/12)} * 440 = 440 \\text{ Hz} = A4\\)\n\\(n = 1 \\rightarrow 2^{(1/12)} * 440 \\approx 466.163 \\text{ Hz} = A \\sharp 4 / B\\flat 4\\)\n\\(n = 12 \\rightarrow 2^{(12/12)} * 440 = 880 \\text{ Hz} = A5\\)\n\\(n = -12 \\rightarrow 2^{(-12/12)} * 440 = 220 \\text{ Hz} = A3\\)\n\n\nNotice that doubling the frequency represents an octave increase of a music note. Now, if we assume that 20 kHz is the maximum frequency that the human ear can hear, then the highest named musical note can be found by solving for \\(n\\) when \\(f =  20 \\text{ kHz}\\).\n\\[\n\\begin{align*}\n20000 &= 2^{\\frac{n}{12}} \\cdot 440 \\\\\n\\frac{20000}{440} &= 2^{\\frac{n}{12}} \\\\\n45.45 &= 2^{\\frac{n}{12}} \\\\\n\\log_2(45.45) &= \\frac{n}{12} \\\\\nn &= 12 \\cdot \\log_2(45.45) \\\\\nn &\\approx 66.0745\n\\end{align*}\n\\]\nWhen \\(n \\approx 66\\), this is 66 semitones aboves A4 and corresponds to the note \\(D \\sharp 10/ E\\flat 10\\).\nLet’s take a listen of the music notes, starting from A3 to D#10!\n\n\nCode\nfs = 44100\nnote_duration = 0.5\npause_duration = 0.10\n\nN = np.arange(int(fs * note_duration))\nt = N / fs\n\npause_arr = np.zeros(int(fs * pause_duration))\n\ndef get_frequency(n):\n    \"\"\"\n    Return the frequency of the note n semitones starting from A4 (440 Hz).\n    \"\"\"\n    return 2**(n / 12) * 440\n\n\n\naudio_sequence = np.array([])\nfor n in range(-12, 67):\n  freq = get_frequency(n)\n  signal_arr = np.cos(2 * np.pi * freq * t)\n  audio_sequence = np.concatenate((audio_sequence, signal_arr, pause_arr))\n\nAudio(data=audio_sequence, rate=fs)\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/signal_review/index.html#complex-numbers",
    "href": "posts/signal_review/index.html#complex-numbers",
    "title": "GSOC 2025 - Pitch Correction for Sound Playback in Sequencer Blog",
    "section": "Complex Numbers",
    "text": "Complex Numbers\nA complex number \\(z\\) is written in the form:\n\\[\nz = x + jy\n\\]\nwhere \\(x\\) is the real part and \\(y\\) is the imaginary part. Furthermore, \\(j\\) is the imaginary unit, where \\(j = \\sqrt{-1}\\) and \\(j^2 = -1\\).\nIn Python, we can represent complex numbers like so:\n\nz = 2 + 3j\n\nPlotting the complex number \\(z\\) on the coordinate plane, we have:\n\n\nCode\nfig, ax = plt.subplots(figsize=(4, 4))\nax.set_xlim(-1, 5)\nax.set_ylim(-1, 5)\nplt.axhline(y=0, color='black', linewidth=1.5)  \nplt.axvline(x=0, color='black', linewidth=1.5) \n\nplt.grid(True)\nplt.scatter(z.real, z.imag)\nplt.text(z.real, z.imag, f'({z.real}, {z.imag}j)', fontsize=12, verticalalignment='bottom')\nplt.xlabel('Real axis')\nplt.ylabel('Imaginary axis')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe conjugation of \\(z\\) is represented by the negation of the imaginary part, and is denoted by:\n\\[\nz^* = x - jy\n\\]\nIn Python, we can get the conjugate by using .conjugate() method:\n\nz_conj = z.conjugate()\nprint(z_conj)\n\n(2-3j)\n\n\nPlotting the conjugation, we have:\n\n\nCode\nfig, ax = plt.subplots(figsize=(4, 4))\nax.set_xlim(-1, 5)\nax.set_ylim(-5, 5)\nplt.axhline(y=0, color='black', linewidth=1.5)  \nplt.axvline(x=0, color='black', linewidth=1.5) \n\nplt.grid(True)\nplt.scatter(z.real, z.imag)\nplt.text(z.real, z.imag, f'  z: ({z.real}, {z.imag}j)', fontsize=12, verticalalignment='bottom')\n\nplt.scatter(z_conj.real, z_conj.imag, color='red', label='z_conj')\nplt.text(z_conj.real, z_conj.imag, f'  z*: ({z_conj.real}, {z_conj.imag}j)', fontsize=12, verticalalignment='top')\n\n\nplt.xlabel('Real axis')\nplt.ylabel('Imaginary axis')\nplt.show()\n\n\n\n\n\n\n\n\n\nSo geometrically, the conjugation can represent reflection over the real axis!\n\nEuler’s Formula\nEuler’s formula is represented as: \\[\ne^{j\\theta} = \\cos(\\theta) + j\\sin(\\theta)\n\\]\nAny complex number \\(z\\) can be represented in polar coordinates like so:\n\\[\n\\begin{align*}\nz &= r \\cdot \\left(\\cos(\\theta) + \\mathrm{j}\\cdot\\sin(\\theta) \\right)\\\\\n  &= r \\cdot e^{\\mathrm{j}\\theta}\n\\end{align*}\n\\]\nwhere \\(r\\) represents the magnitude of the complex number and \\(\\theta\\) represents the\n\n\nCode\nr1, theta1 = 2, np.pi / 4  \nr2, theta2 = 3, np.pi / 6\nz1 = r1 * np.exp(1j * theta1)\nz2 = r2 * np.exp(1j * theta2)\n\nz_product = z1 * z2\n\nr_product = np.abs(z_product)\ntheta_product = np.angle(z_product)\n\nprint(f\"z1 = {z1:.2f} (Retangular Form)\")\nprint(f\"z2 = {z2:.2f} (Retangular Form)\")\nprint(f\"z1 Angle (radians): {np.angle(z1):.2f}\")\nprint(f\"z1 * z2 = {z_product:.2f}\")\nprint(f\"Product magnitude: {r_product:.2f}\")\nprint(f\"Product angle (radians): {theta_product:.2f}\")\nprint(f\"Product angle (degrees): {np.degrees(theta_product):.2f}\")\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, polar=True)\n\nax.plot([0, theta1], [0, r1], label='z1', marker='o')\nax.plot([0, theta2], [0, r2], label='z2', marker='o')\nax.plot([0, theta_product], [0, r_product], label='z1 * z2', marker='o', linestyle='--')\nax.set_rlabel_position(135)\n\n\nax.set_title(\"Polar Plot of z1, z2, and z1 * z2\", va='bottom')\nax.legend(loc='upper right')\nplt.show()\n\n\nz1 = 1.41+1.41j (Retangular Form)\nz2 = 2.60+1.50j (Retangular Form)\nz1 Angle (radians): 0.79\nz1 * z2 = 1.55+5.80j\nProduct magnitude: 6.00\nProduct angle (radians): 1.31\nProduct angle (degrees): 75.00"
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html",
    "href": "posts/blender_sequencer_audio/index.html",
    "title": "Understanding VSE Audio in the Sequencer",
    "section": "",
    "text": "This week, I have been taking the time to understand how Audaspace is integrated into Blender’s VSE. The most important files are blenkernel/intern/sound.cc and its header file blenkernel/BKE_sound.h. I will taking a little look at specifically animating sound properties (eg. volume and pitch) which @iss or Richard Antalik describes in the following thread. Furthermore, @neYyon or Jörg Müller gave tips on how to integrate the Rubber Band Library into Audaspace here."
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html#playing-audio",
    "href": "posts/blender_sequencer_audio/index.html#playing-audio",
    "title": "Understanding VSE Audio in the Sequencer",
    "section": "Playing Audio",
    "text": "Playing Audio\nWhen you press Spacebar to play an audio clip in the sequencer, the BKE_sound_play_scene function is called, which makes several calls to the Audaspace library below.\n\n\nCode\n\n\n  // Called by `wmOperatorStatus ED_screen_animation_play(bContext *C, int sync, int mode)` in `screen_cops.cc`\n  void BKE_sound_play_scene(Scene *scene)\n  {\n    std::lock_guard lock(g_state.sound_device_mutex);\n    sound_device_use_begin();\n    sound_verify_evaluated_id(&scene-&gt;id);\n\n    AUD_Status status;\n    const double cur_time = get_cur_time(scene);\n\n    AUD_Device_lock(g_state.sound_device);\n\n    if (scene-&gt;sound_scrub_handle &&\n        AUD_Handle_getStatus(scene-&gt;sound_scrub_handle) != AUD_STATUS_INVALID)\n    {\n      /* If the audio scrub handle is playing back, stop to make sure it is not active.\n      * Otherwise, it will trigger a callback that will stop audio playback. */\n      AUD_Handle_stop(scene-&gt;sound_scrub_handle);\n      scene-&gt;sound_scrub_handle = nullptr;\n      /* The scrub_handle started playback with playback_handle, stop it so we can\n      * properly restart it. */\n      AUD_Handle_pause(scene-&gt;playback_handle);\n    }\n\n    status = scene-&gt;playback_handle ? AUD_Handle_getStatus(scene-&gt;playback_handle) :\n                                      AUD_STATUS_INVALID;\n\n    if (status == AUD_STATUS_INVALID) {\n      sound_start_play_scene(scene);\n\n      if (!scene-&gt;playback_handle) {\n        AUD_Device_unlock(g_state.sound_device);\n        return;\n      }\n    }\n\n    if (status != AUD_STATUS_PLAYING) {\n      /* Seeking the synchronizer will also seek the playback handle.\n      * Even if we don't have A/V sync on, keep the synchronizer and handle seek time in sync. */\n      AUD_seekSynchronizer(cur_time);\n      AUD_Handle_setPosition(scene-&gt;playback_handle, cur_time);\n      AUD_Handle_resume(scene-&gt;playback_handle);\n    }\n\n    if (scene-&gt;audio.flag & AUDIO_SYNC) {\n      AUD_playSynchronizer();\n    }\n\n    AUD_Device_unlock(g_state.sound_device);\n  }"
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html#animating-audio",
    "href": "posts/blender_sequencer_audio/index.html#animating-audio",
    "title": "Understanding VSE Audio in the Sequencer",
    "section": "Animating Audio",
    "text": "Animating Audio\nThere are currently 5 properties of audio that can be animated in Audiospace shown below:\n\n\nCode\n\nAnimateableProperty* SequenceEntry::getAnimProperty(AnimateablePropertyType type)\n{\n    switch(type)\n    {\n    case AP_VOLUME:\n        return &m_volume;\n    case AP_PITCH:\n        return &m_pitch;\n    case AP_PANNING:\n        return &m_panning;\n    case AP_LOCATION:\n        return &m_location;\n    case AP_ORIENTATION:\n        return &m_orientation;\n    default:\n        return nullptr;\n    }\n}\n\nWe’ll be looking at in particular how the volume and pitch is animated in Blender’s VSE."
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html#volume",
    "href": "posts/blender_sequencer_audio/index.html#volume",
    "title": "Understanding VSE Audio in the Sequencer",
    "section": "Volume",
    "text": "Volume\n\n(CAUTION: VOLUME WARNING)\n\n\n\n\n\n\nFigure 1\n\n\n\n\nThe RNA for the volume property is defined rna_sequencer.cc in the function rna_def_audio_options\n\n\nCode\n\nstatic void rna_def_audio_options(StructRNA *srna)\n{\n  PropertyRNA *prop;\n\n  prop = RNA_def_property(srna, \"volume\", PROP_FLOAT, PROP_NONE);\n  RNA_def_property_float_sdna(prop, nullptr, \"volume\");\n  RNA_def_property_range(prop, 0.0f, 100.0f);\n  RNA_def_property_float_default(prop, 1.0f);\n  RNA_def_property_ui_text(prop, \"Volume\", \"Playback volume of the sound\");\n  RNA_def_property_translation_context(prop, BLT_I18NCONTEXT_ID_SOUND);\n  RNA_def_property_update(prop, NC_SCENE | ND_SEQUENCER, \"rna_Strip_audio_update\");\n}\n\nAnd the UI for the volume (or the sound properties) is defined in space_sequencer.py\n\n\nCode\n...\ndef draw(self, context):\n        layout = self.layout\n\n        st = context.space_data\n        overlay_settings = st.timeline_overlay\n        strip = context.active_strip\n        sound = strip.sound\n\n        layout.active = not strip.mute\n\n        if sound is not None:\n            layout.use_property_split = True\n            col = layout.column()\n\n            split = col.split(factor=0.4)\n            split.alignment = 'RIGHT'\n            split.label(text=\"Volume\", text_ctxt=i18n_contexts.id_sound)\n            split.prop(strip, \"volume\", text=\"\")\n\n            layout.use_property_split = False\n        ...\n\n\nWhen we scrub the volume property, the function BKE_sound_set_scene_sound_volume_at_frame is called. For example below, sliding the volume to 1.4…\n\nleads to the breakpoint in BKE_sound_set_scene_sound_volume_at_frame.\n\nThe frame variable corresponds to the location of the playhead at 2 seconds (60 frames) and 29 frames, so (60 + 29 = 89 frames). The handle variable is a pointer to Audaspace’s AUD_SequenceEntry class, which stores the variables such as volume and pitch for a sound sequence as shown below in Audaspace’s SequenceEntry.h.\n\n\nCode\n\n    /// The animated volume.\n    AnimateableProperty m_volume;\n\n    /// The animated panning.\n    AnimateableProperty m_panning;\n\n    /// The animated pitch.\n    AnimateableProperty m_pitch;\n\n    /// The animated location.\n    AnimateableProperty m_location;\n\n    /// The animated orientation.\n    AnimateableProperty m_orientation;\n\nThe BKE_sound_set_scene_sound_volume_at_frame function just only makes a call to AUD_SequenceEntry_setAnimationData as shown below:\n\n\nCode\n\nvoid BKE_sound_set_scene_sound_volume_at_frame(void *handle,\n                                               const int frame,\n                                               float volume,\n                                               const char animated)\n{\n  AUD_SequenceEntry_setAnimationData(handle, AUD_AP_VOLUME, frame, &volume, animated);\n}\n\nThe function AUD_SequenceEntry_setAnimationData in AUD_Sequence.cpp looks like the following:\n\n\nCode\n\nAUD_API void AUD_SequenceEntry_setAnimationData(AUD_SequenceEntry* entry, AUD_AnimateablePropertyType type, int frame, float* data, char animated)\n{\n    AnimateableProperty* prop = (*entry)-&gt;getAnimProperty(static_cast&lt;AnimateablePropertyType&gt;(type));\n    if(animated)\n    {\n        if(frame &gt;= 0)\n            prop-&gt;write(data, frame, 1);\n    }\n    else\n    {\n        prop-&gt;write(data);\n    }\n}\n\n\nAnd finally, below is the call stack for BKE_sound_set_scene_sound_volume_at_frame\n\n\nCall Stack\n\nBlender!BKE_sound_set_scene_sound_volume_at_frame(void*, int, float, char) (blender/source/blender/blenkernel/intern/sound.cc:1029)\nBlender!blender::seq::strip_update_sound_properties(Scene const*, Strip const*) (blender/source/blender/sequencer/intern/sequencer.cc:997)\nBlender!blender::seq::strip_sound_update_cb(Strip*, void*) (blender/source/blender/sequencer/intern/sequencer.cc:1083)\nBlender!blender::seq::strip_for_each_recursive(ListBase*, bool (*)(Strip*, void*), void*) (blender/source/blender/sequencer/intern/iterator.cc:29)\nBlender!blender::seq::for_each_callback(ListBase*, bool (*)(Strip*, void*), void*) (blender/source/blender/sequencer/intern/iterator.cc:44)\nBlender!blender::seq::eval_strips(Depsgraph*, Scene*, ListBase*) (blender/source/blender/sequencer/intern/sequencer.cc:1092)\nBlender!blender::deg::DepsgraphNodeBuilder::build_scene_sequencer(Scene*)::$_0::operator()(Depsgraph*) const (blender/source/blender/depsgraph/intern/builder/deg_builder_nodes.cc:2312)\n\nThe important thing to note above is that BKE_sound_set_scene_sound_volume_at_frame is called by the function strip_update_sound_properties in sequencer.cc. In a future blog, I’ll likely discuss about the animated parameter in AUD_SequenceEntry_setAnimationData and have a demo program to test out, as currently, I’m not exactly sure what it does and haven’t investigated thoroughly yet!"
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html#pitch",
    "href": "posts/blender_sequencer_audio/index.html#pitch",
    "title": "Understanding VSE Audio in the Sequencer",
    "section": "Pitch",
    "text": "Pitch\nThis GSOC project will primarily focus on the sound property of pitch for implementing pitch correction. In Blender, pitch is primarily affected by retiming keys, which allows you to change the playback speed of video/audio clips. However, this has the consequence of increasing the pitch when the playback speed is increased or decreasing the pitch when the playback speed is decreased. Below is an example of using retiming keys to increase the playback audio speed.\n\nFirst, the code relevant to the drawing the retiming keys onto the audio strip can be found in sequencer_retiming_draw.cc but I haven’t had the time look into, but I believe it is not currently not too relevant for this project.\nNow, one of the relevant function related to the functionality of the retiming keys (and therefore pitch!) is retiming_sound_animation_data_set in strip_retiming.cc which is declared as the following:\n\n\nCode\n\nvoid retiming_sound_animation_data_set(const Scene *scene, const Strip *strip)\n{\n  /* Content cut off by `anim_startofs` is as if it does not exist for sequencer. But Audaspace\n   * seeking relies on having animation buffer initialized for whole sequence. */\n  if (strip-&gt;anim_startofs &gt; 0) {\n    const int strip_start = time_start_frame_get(strip);\n    BKE_sound_set_scene_sound_pitch_constant_range(\n        strip-&gt;scene_sound, strip_start - strip-&gt;anim_startofs, strip_start, 1.0f);\n  }\n\n  const float scene_fps = float(scene-&gt;r.frs_sec) / float(scene-&gt;r.frs_sec_base);\n  const int sound_offset = time_get_rounded_sound_offset(strip, scene_fps);\n\n  RetimingRangeData retiming_data = strip_retiming_range_data_get(scene, strip);\n  for (int i = 0; i &lt; retiming_data.ranges.size(); i++) {\n    RetimingRange range = retiming_data.ranges[i];\n    if (range.type == TRANSITION) {\n\n      const int range_length = range.end - range.start;\n      for (int i = 0; i &lt;= range_length; i++) {\n        const int frame = range.start + i;\n        BKE_sound_set_scene_sound_pitch_at_frame(\n            strip-&gt;scene_sound, frame + sound_offset, range.speed_table[i], true);\n      }\n    }\n    else {\n      BKE_sound_set_scene_sound_pitch_constant_range(\n          strip-&gt;scene_sound, range.start + sound_offset, range.end + sound_offset, range.speed);\n    }\n  }\n}\n\n\nThe code above loops over retiming key ranges (which is stored in the RetimingRange class and contains variables like the start and end frame, the playbeed speed, as well as it type which is defined as an enumerator below in strip_retiming.cc)\nenum eRangeType {\n  LINEAR = 0,\n  TRANSITION = 1,\n};\nAdditionally, retiming_sound_animation_data_set makes a call to two different pitch functions BKE_sound_set_scene_sound_pitch_at_frame and BKE_sound_set_scene_sound_pitch_constant_range depending on whether the range is LINEAR or TRANSITION and is defined as below in sound.cc:\n\n\nCode\n\nvoid BKE_sound_set_scene_sound_pitch_at_frame(void *handle,\n                                              const int frame,\n                                              float pitch,\n                                              const char animated)\n{\n  AUD_SequenceEntry_setAnimationData(handle, AUD_AP_PITCH, frame, &pitch, animated);\n}\n\nvoid BKE_sound_set_scene_sound_pitch_constant_range(void *handle,\n                                                    int frame_start,\n                                                    int frame_end,\n                                                    float pitch)\n{\n  frame_start = max_ii(0, frame_start);\n  frame_end = max_ii(0, frame_end);\n  AUD_SequenceEntry_setConstantRangeAnimationData(\n      handle, AUD_AP_PITCH, frame_start, frame_end, &pitch);\n}\n\nBelow is an example of the two range types:\n\nwhere the TRANSITION range is represented by the retiming keys with 77% - 116% in between while the retiming keys with 77% or 116% represents a LINEAR range. In the example image above, the TRANSITION range interpolates between the 77% and 116% playback speed from the 00:23-00:36 range. Codewise, RetimingRange stores the interpolated values from 77% - 116% inside a vector speed_table which is set for which is set for each frame within the TRANSITION range.\nMeanwhile, the LINEAR ranges (00:00-00:23 and 00:36-01:10) maintains a constant playback speed (and thus the same constant pitch for that particular playback speed).\nHere’s an example audio clip which contains both the RetimingRanges type!"
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html#next-steps",
    "href": "posts/blender_sequencer_audio/index.html#next-steps",
    "title": "Understanding VSE Audio in the Sequencer",
    "section": "Next Steps",
    "text": "Next Steps\n\nIn the upcoming weeks, I will probably play around with the AUD_SequenceEntry class in an isolated environment to get a better sense of how it works and also mess around with the different AnimateablePropertys. - I currently have the Rubberband Library built as a static library in a separate, local fork, and make it public for code review this or next week. I will also take the time to understand Audaspace’s Effect and EffectReader class by looking at the many, many examples and see if I can do something with the Rubberband Library there\nI also need to finish up writing the post about compiling the Rubberband Library, how to use it, and benchmarking it to make sure it is suitable and usable for Audaspace/Blender"
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html#questions",
    "href": "posts/blender_sequencer_audio/index.html#questions",
    "title": "Understanding VSE Audio in the Sequencer",
    "section": "Questions",
    "text": "Questions\nThese were things/questions I wasn’t sure about Blender’s codebase or where I can access the variable from the UI in the 1st week! These were answered by Aras, which I summarized his answers below.\n\nQuestion #1\nFor this line in sequencer.cc\nBKE_sound_set_scene_sound_volume_at_frame(strip-&gt;scene_sound, frame, strip-&gt;volume, (strip-&gt;flag & SEQ_AUDIO_VOLUME_ANIMATED) != 0);\nwhere can the strip flag be toggled for having animated volume in Blender\n\n\nAnswer #1\nThe volume property is driven by an animation f-curve or animation driver expression.\n\n\n\nQuestion #2\nFor AUD_SequenceEntry_setAnimationData function what exactly does the animated parameter do? I didn’t have time to look into further this week.\n\n\nAnswer #2\nWhen the animated parameter is set to true, then it applies whatever property to that given frame. Otherwise, it sets it for that entire SequenceEntry\n\n\nQuestion #3\nFor the operator types below, what editor has these operators in Blender?\n\n\nCode\n\nvoid ED_operatortypes_sound()\n{\n  WM_operatortype_append(SOUND_OT_open);\n  WM_operatortype_append(SOUND_OT_open_mono);\n  WM_operatortype_append(SOUND_OT_mixdown);\n  WM_operatortype_append(SOUND_OT_pack);\n  WM_operatortype_append(SOUND_OT_unpack);\n  WM_operatortype_append(SOUND_OT_update_animation_flags);\n  WM_operatortype_append(SOUND_OT_bake_animation);\n}\n\n\nAnswer #3\nIt’s from “Render -&gt; Render Audio…” menu item in Blender, but the operations are not too relevant to the project"
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html#there-were-several-things",
    "href": "posts/blender_sequencer_audio/index.html#there-were-several-things",
    "title": "Understanding VSE Audio",
    "section": "There were several things",
    "text": "There were several things"
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html#thoughts-so-far",
    "href": "posts/blender_sequencer_audio/index.html#thoughts-so-far",
    "title": "Understanding VSE Audio in the Sequencer",
    "section": "Thoughts So Far",
    "text": "Thoughts So Far"
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html#thoughts",
    "href": "posts/blender_sequencer_audio/index.html#thoughts",
    "title": "Understanding VSE Audio in the Sequencer",
    "section": "Thoughts",
    "text": "Thoughts\n\nI hope these notes are useful for"
  },
  {
    "objectID": "posts/blender_sequencer_audio/index.html#random-thoughts",
    "href": "posts/blender_sequencer_audio/index.html#random-thoughts",
    "title": "Understanding VSE Audio in the Sequencer",
    "section": "Random Thoughts",
    "text": "Random Thoughts\n\nI learned a lot of things during this first week from things like compiling/making a blog, to learning a little bit about CMAKE files and how to read them, and to using the debugger in VSCode more efficiently and for other projects\nI hope these notes were somewhat useful or insightful to anyone looking to understand a bit about Blender’s VSE! I know these notes will be useful for me down the line as I start to implement the pitch correction toggle, and I will have more questions down the line! This post took way longer to write than I initially thought, but I really enjoyed taking the time to understand and explain a portion of Blender’s VSE.\nIt so far seems very likely that I will not have to manually implement the pitch correction algorithm myself!\nIn the future, I will probably update this post to have more information, as I’ve just mostly touched upon areas that were relevant to the project"
  }
]